# dubbo server yaml configure file


# application config
application:
  name : "user-info-server"


config_center:
  protocol: "zookeeper"
  address: "127.0.0.1:2181"

registries :
  "demoZk":
    protocol: "zookeeper"
    timeout	: "3s"
    address: "127.0.0.1:2181,127.0.0.1:2182,127.0.0.1:2183"

services:
  "UserProvider": # 和服务 Reference() 返回值保持一致,此为标识该接口的 key
    registry: "demoZk"
    interface : "com.ikurento.user.UserProvider"

protocol_conf:
  # 这里是协议独立的配置，在dubbo协议下，大多数配置即为getty session相关的配置。
  dubbo:
    # 一个session会始终保证connection_number个tcp连接个数，默认是16，
    # 但这里建议大家配置相对小的值，一般系统不需要如此多的连接个数。
    # 每隔reconnect_interval时间，检查连接个数，如果小于connection_number，
    # 就建立连接。填0或不填都为默认值300ms
    reconnect_interval: 0
    connection_number: 2
    # 客户端发送心跳的间隔
    heartbeat_period: "30s"

    session_number: 700
    # OnCron时session的超时时间，超过session_timeout无返回就关闭session
    session_timeout: "180s"
    # 每一个dubbo interface的客户端，会维护一个最大值为pool_size大小的session池。
    # 每次请求从session池中select一个。所以真实的tcp数量是session数量*connection_number，
    # 而pool_size是session数量的最大值。测试总结下来一般程序4个tcp连接足以。
    pool_size: 4
    # session保活超时时间，也就是超过session_timeout时间没有使用该session，就会关闭该session
    pool_ttl: 600
    # 处理返回值的协程池大小
    gr_pool_size: 1200
    # 读数据和协程池中的缓冲队列长度，目前已经废弃。不使用缓冲队列
    queue_len: 64
    queue_number: 60
    getty_session_param:
      compress_encoding: false
      tcp_no_delay: true
      tcp_keep_alive: true
      keep_alive_period: "120s"
      tcp_r_buf_size: 262144
      tcp_w_buf_size: 65536
      pkg_rq_size: 1024
      pkg_wq_size: 512
      tcp_read_timeout: "1s"  # 每次读包的超时时间
      tcp_write_timeout: "5s" # 每次写包的超时时间
      wait_timeout: "1s"
      max_msg_len: 1024000 # 最大数据传输长度
      session_name: "server"